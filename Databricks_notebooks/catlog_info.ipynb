{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70948cf-3acf-41fe-b5ae-fad167941985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Example of extracting schema information using Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ExportOlympicCatalog\").getOrCreate()\n",
    "\n",
    "# Set the correct catalog\n",
    "spark.sql(\"USE CATALOG olympics\")\n",
    "\n",
    "# List all databases\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "\n",
    "# Iterate through each database and list tables\n",
    "for db in databases:\n",
    "    db_name = db['databaseName']\n",
    "    tables = spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
    "    \n",
    "    for table in tables:\n",
    "        table_name = table['tableName']\n",
    "        schema = spark.sql(f\"DESCRIBE {db_name}.{table_name}\").collect()\n",
    "        \n",
    "        # Print or save the schema information\n",
    "        print(f\"Schema for {db_name}.{table_name}:\")\n",
    "        for column in schema:\n",
    "            print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba79fb53-4544-4624-9a28-139fbee8c13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# Use dbutils to handle DBFS file operations\n",
    "dbutils.fs.mkdirs('/mnt')\n",
    "\n",
    "output = io.StringIO()\n",
    "schema_writer = csv.writer(output)\n",
    "schema_writer.writerow(['Database', 'Table', 'Column', 'DataType', 'Comment'])\n",
    "\n",
    "for db in databases:\n",
    "    db_name = db['databaseName']\n",
    "    tables = spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
    "    \n",
    "    for table in tables:\n",
    "        table_name = table['tableName']\n",
    "        schema = spark.sql(f\"DESCRIBE {db_name}.{table_name}\").collect()\n",
    "        \n",
    "        for column in schema:\n",
    "            schema_writer.writerow([db_name, table_name, column['col_name'], column['data_type'], column['comment']])\n",
    "\n",
    "csv_content = output.getvalue()\n",
    "dbutils.fs.put('/mnt/exported_olympic_schemas.csv', csv_content, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9498c72e-22a0-4e44-8ecd-89aad151e8b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List the files in the /mnt directory to verify the CSV file location\n",
    "display(dbutils.fs.ls('/mnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18d59c6-bfb8-4c7e-b89c-b5dedf159f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Download the file from DBFS to the local file system\n",
    "local_path = '/tmp/exported_olympic_schemas.csv'\n",
    "dbfs_path = '/mnt/exported_olympic_schemas.csv'\n",
    "\n",
    "# Copy the file from DBFS to the local file system\n",
    "dbutils.fs.cp(dbfs_path, 'file:/Desktop')\n",
    "\n",
    "# Display the local file path for download\n",
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6dd2e3-83b0-447c-9def-7236b3ef487a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Download the file from DBFS to the local file system\n",
    "local_path = '/tmp/exported_olympic_schemas.csv'\n",
    "dbfs_path = '/mnt/exported_olympic_schemas.csv'\n",
    "\n",
    "# Copy the file from DBFS to the local file system\n",
    "dbutils.fs.cp(dbfs_path, local_path)\n",
    "\n",
    "# Display the local file path for download\n",
    "local_path"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "catlog_info",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
